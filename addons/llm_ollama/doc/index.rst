==========================================
Ollama Provider for Odoo LLM Integration
==========================================

Local AI deployment with Ollama - privacy-focused, no API costs.

**Module Type:** ðŸ”§ Provider (Local/Privacy-Focused)

Architecture
============

::

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    Used By (Any LLM Module)                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚llm_assistantâ”‚  â”‚llm_thread â”‚  â”‚llm_knowledgeâ”‚  â”‚llm_generateâ”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚          â˜… llm_ollama (This Module) â˜…         â”‚
              â”‚           Ollama Provider (Local AI)          â”‚
              â”‚  ðŸ”’ Llama 3 â”‚ Mistral â”‚ CodeLlama â”‚ Phi â”‚ etc â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â–¼                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           llm             â”‚   â”‚      Ollama Server        â”‚
    â”‚    (Core Base Module)     â”‚   â”‚   (localhost:11434)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Installation
============

What to Install
---------------

**For local AI chat (no external API):**

.. code-block:: bash

    # 1. Install Ollama on your server first
    curl -fsSL https://ollama.ai/install.sh | sh
    ollama pull llama3

    # 2. Install the Odoo module
    odoo-bin -d your_db -i llm_assistant,llm_ollama

Why Choose Ollama?
------------------

+-------------+----------------------+-------------------+
| Feature     | Ollama               | Cloud Providers   |
+=============+======================+===================+
| **Privacy** | ðŸ”’ Data stays local  | â˜ï¸ Sent to cloud  |
+-------------+----------------------+-------------------+
| **Cost**    | ðŸ’° Free (your hardware) | ðŸ’³ Pay per token|
+-------------+----------------------+-------------------+
| **Offline** | âœ… Works offline     | âŒ Requires internet|
+-------------+----------------------+-------------------+

Common Setups
-------------

+---------------------------+----------------------------------------------+
| I want to...              | Install                                      |
+===========================+==============================================+
| Local AI chat             | ``llm_assistant`` + ``llm_ollama``           |
+---------------------------+----------------------------------------------+
| Local AI + RAG            | Above + ``llm_knowledge`` + ``llm_pgvector`` |
+---------------------------+----------------------------------------------+
| Mixed (local + cloud)     | Install both ``llm_ollama`` + ``llm_openai`` |
+---------------------------+----------------------------------------------+

Features
========

- Connect to Ollama with proper configuration
- Support for various open-source models (Llama, Mistral, etc.)
- Text generation capabilities
- Function calling support
- Automatic model discovery
- Local deployment for privacy and control

Configuration
=============

1. Install Ollama on your server
2. Navigate to **LLM > Configuration > Providers**
3. Create provider with URL (default: http://localhost:11434)
4. Click "Fetch Models" to import available models

Technical Specifications
========================

- **Version**: 18.0.1.1.0
- **License**: LGPL-3
- **Dependencies**: ``llm``
- **Python Package**: ``ollama``

Related Modules
===============

- **``llm``** - Core infrastructure
- **``llm_assistant``** - AI assistants
- **``llm_openai``** - Alternative: OpenAI
- **``llm_mistral``** - Alternative: Mistral AI

License
=======

LGPL-3

----

*Â© 2025 Apexive Solutions LLC*
